---
layout: post
title:  "How to: Use and understand XGBoost and Random Forest Classifiers"
date:   2016-12-18 19:45:31 +0530
categories: data_science
author: "wnowak10"
---

If you look at [Kaggle](kaggle.com) competitions, you can quick glean that XGBoost (in Python or R) is hugely popular and successful in solving classification problems.

I am familiar with the idea of a decision tree, which is powerful in its simplicity. But the added complexity of something like XGBoost is a bit beyond me. In particular, I don't fully understand:

Algorithms that power a plain old, vanilla decision tree.


[I want this to link to foo](#algorithms)

{: id="algorithms"}
### Foo are you?

<!-- - [Algorithms](#algorithms) -->
- Why random forests are usefu
- How gradient boosting applies
- What bagging is

So, here's the plan. 

1. If you need a primer on how a decision tree works, there is a lot of good extant content on this. Check out this simple [video](https://www.youtube.com/watch?v=eKD5gxPPeY0) for an explanation of the basic idea. 

2. Assuming the decision trees are understood, I am going to address the questions above, while also talking about the process through which I do so. 

So, let's begin... 
<br>
<br>
-----
<br>
<br>

# Algorithms

My understanding is that vanilla decision trees work by looking at data (which can be categorical or quantitive in nature), and finding the most information rich split. For example, if I were trying to classify morning beverage using "Temperature" and "Cloud cover" as classifiers (as shown in the following table)
<br>
<br>

<a>
	<img src="/images/coffeetable.png" alt="Drawing" style="width: 357px; height: 365px"/>
</a>
<br>
<br>

What would be the optimal first split for the tree?
<br>
<br>

<a>
	<img src="/images/keep-calm-and-think-about-it-17.png" alt="Drawing" style="width: 600px; height: 700px"/>
</a>
<br>
<br>

Actually, did you think about it? Hopefully.

We can see that splitting on the temp is the best call, as if temps are below 80, we can correctly bin every beverage choice. We don't even need the information provided about the cloud cover. 
<br>
<br>

<a>
	<img src="/images/classified_coffee.png" alt="Drawing" style="width: 350; height: 350"/>
</a>

<br>
<br>
This basic split shows the "tree" idea:

<br>
<br>

<a>
	<img src="/images/basic_tree.png" alt="Drawing" style="width: 550; height: 550"/>
</a>
<br>
<br>

---
<br>
<br>

But how does the computer actually do this?

I spent some time this fall reading David MacKay's [*Infomation Theory, Inference, and Learning Algorithms*](http://www.inference.phy.cam.ac.uk/itprnn/book.pdf). He talks clearly about information and entropy in Chapter 4, and I would recommend reading if these topics are at all of interest. These ideas are surely related to a decision tree, as we're looking to make a split that is most informative at each step along the way.

Let's try to slow down a bit and be more rigorous in our discussion here. \#math! 

Thales Sehn KÃ¶rting discusses this some in this helpful [video](https://www.youtube.com/watch?v=Qdi0GBWrDO8), but I want to expound upon and clarify his ideas here. 


#header


# header with space?