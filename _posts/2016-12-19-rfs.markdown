---
layout: post
title:  "How to: Use (and understand) XGBoost (and Random Forest Classifiers, more broadly)"
date:   2016-12-18 19:45:31 +0530
categories: data_science
author: "wnowak10"
---

If you look at [Kaggle](kaggle.com) competitions, you can quick glean that XGBoost (in Python or R) is hugely popular and successful in solving classification problems.

I am familiar with the idea of a decision tree, which is powerful in its simplicity. But the added complexity of something like XGBoost is a bit beyond me. In particular, I don't fully understand:


- [How do vanilla tree algorithms even work](#algorithms)
- Why random forests are useful and what ensembles are
- How gradient boosting applies
- What bagging is

Mysteries abound! So, here's the plan. 

1. If you need a primer on how a decision tree works, there is a lot of good extant content on this. Check out this simple [video](https://www.youtube.com/watch?v=eKD5gxPPeY0) for an explanation of the basic idea. 

2. Assuming that decision trees are understood, I am going to address the questions above, while also talking about the process through which I do so. I'll try to think through how I might go about solving the problem, and then provide research and some explanation of how the current state of the art works. 

So, let's begin... 
<br>
<br>
-----
<br>
<br>


{: id="algorithms"}
# How do vanilla tree algorithms even work?

(Let's assume, for now, that our trees will only have two branches emanating from each node. Also, assume we are predicting a binary random variable.)

My understanding is that vanilla decision trees work by looking at data (which can be categorical or quantitive in nature), and finding the most information-rich split (more on this later). For example, if I were trying to classify "Beverage" using "Temperature" and "Cloud cover" as classifiers (as shown in the following table),
<br>
<br>

<a>
	<img src="/images/coffeetable.png" alt="Drawing" style="width: 357px; height: 365px"/>
</a>
<br>
<br>

what would be the optimal first split for the tree?
<br>
<br>

<a>
	<img src="/images/keep-calm-and-think-about-it-17.png" alt="Drawing" style="width: 550; height: 550"/>
</a>
<br>
<br>

Actually, did you think about it? Hopefully.

We can see that splitting on the temp is the best call, as if temps are below 80, we can correctly bin every beverage choice. We don't even need the information provided about the cloud cover. 
<br>
<br>

{: id="table"}
<a>
	<img src="/images/classified_coffee.png" alt="Drawing" style="width: 350; height: 350"/>
</a>

<br>
<br>
This basic split shows the "tree" idea:

<br>
<br>

<a>
	<img src="/images/basic_tree.png" alt="Drawing" style="width: 550; height: 550"/>
</a>

<br>

To sum up, [Wikipedia](https://en.wikipedia.org/wiki/C4.5_algorithm) gives some semi-helpful pseudo-code that explains the decision tree process (though the gory and exciting details are absent).

1. Check for the above base cases.
2. For each attribute a, find the normalized information gain ratio from splitting on a.
3. Let a_best be the attribute with the highest normalized information gain.
4. Create a decision node that splits on a_best.
5. Recur on the sublists obtained by splitting on a_best, and add those nodes as children of node.

It still remains to be seen: how does the computer actually do this? In particular, how do they determine that attribute "a" has the highest information gain?


<br>
<br>

---
<br>
<br>

## My ideas...

Let's try to slow down a bit and be more rigorous in our discussion here. \#math! 

Assume that there are p features. There n observations in our training set that we must classify. We need to determine:

\- Which of the p features is best to split on? Where do we split this feature (if it is numeric or categorical with more than 2 outcomes)? (I'll refer to this as the attribute (a) that we split on. That is, referring to an attribute a describes both the feature and the split point of that feature.)

We are trying to classify a label, which can take values 1 or 0. 

(In the example above, we had p = 2 features, and we could easily see that splitting on the temperature feature at some point between 75 and 85 was the right move.

So we need to look at each feature and then describe, numerically, how well this split does in terms of helping us acheive our end goal of classification.

### For numeric variables:

#### Classification of split:

It seems that it'd be wise to sort the data on the numeric variable. Using our example, this would be temperature. Looking at the [table](#table) above, we'd split somewhere like 55 (the median of the temperature set). If we did this, we'd find that 100% of beverages below 55 are coffee, and 60% (3/5) of beverages above 55 degrees are coffee. 

<a>
	<img src="/images/class.png" alt="Split" style="width: 550; height: 550"/>
</a>

Ideally, in a pure split, we'd have both nodes of the split at 100% (this would mean there is no ambiguity in the classification). 

One idea for a helpful numerical descriptor could be s, where s = b1 + b2, where b1 = max(x, 1-x) with x = sum(l<sub>i</sub>)/(c<sub>1</sub>), where l<sub>i</sub> is the ith label and c<sub>1</sub> is the number of elements in branch 1 (c for cardinality). b2 is defined similarly for the second branch. 

In this case, splitting on feature "Temperature" at the median, we find that s<sub>a<sub>Q2</sub></sub> = 1 + .6 = 1.6. It seems that an ideal split would result in two pure splits, leaving us with s = 1 + 1 = 2. On the contrary, a non-informative split would do no better than chance. In binary classification, this should result in 50% success along each branch. So, we have 1 < s < 2. 

Pseudo-code for the optimal classification for a numeric feature would look something like:

0. Set m = 0.
1. Sort data by numeric feature
2. Set attribute = median (Q2).
3. Calculate s for this split. If s > m: m = s. If s < m, stop.
4. If s does not equal 2, test a new attribute according to rule r. 
5. Repeat 3 and 4. 


**RULE R**
<pre><code>
*If b1 > b2, move test new attribute = median of upper half of data. If b1 < b2, move to new attribute = median of lower half of data. 
</code></pre>

Let's continue with the coffee and tea example. Following rule r, we'd choose a new attribute which is the median of upper half (since b1>b2). In this case, the median is 75. If we bin everything up to and including 75, we get the following.

<a>
	<img src="/images/classified_coffee.png" alt="Drawing" style="width: 350; height: 350"/>
</a>

Now, we find that s = 1 + 1 = 2. Since we have reached the maximum 2, we can stop our search. 

#### Split choice:

In addition to judging our split attribute on its ability to successfully classify, we can generate another metric to judge various split attributes.  

A good attribute splits data evenly. In example 1 below, the attribute acheives an s score of 1 + .89 = 1.89. However, it doesn't split the data very evenly.

\- Example 1: 

<a>
	<img src="/images/loweven.png" alt="Drawing" style="width: 350; height: 350"/>
</a>

In example 2, we have a worse split, in that s = .8 + .6 = 1.4. 

\- Example 2:

<a>
	<img src="/images/higheven.png" alt="Drawing" style="width: 350; height: 350"/>
</a>

However, we are "doing more work" here. Given that we are trying to productively classify all data points, an attribute that produces a more even split should be prized for its efficiency, in some sense. We will have to rely on fewer nodes of a decision tree if each node can more evenly split the data.

We can introduce a new metric. We will call this metric e. e = log(c1) + log(c2). For example 1, we find e<sub>1</sub> = log(1)+log(9) = .954. In example 2, e<sub>2</sub> = log(5)+log(5) = 1.379. 

### For categorical variables


## The way this actually works...



Some further references that touch on these ideas:

Thales Sehn KÃ¶rting discusses this some in this helpful [video](https://www.youtube.com/watch?v=Qdi0GBWrDO8), but I want to expound upon and clarify his ideas here. 

David MacKay's [*Infomation Theory, Inference, and Learning Algorithms*](http://www.inference.phy.cam.ac.uk/itprnn/book.pdf) gives a good explanation of information and entropy, so you can look 



<br>
<br>

---
<br>
<br>


Other useful resources:

- [XGBoost docs](http://xgboost.readthedocs.io/en/latest/model.html)
- [Vidhya](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)
- [Quora](https://www.quora.com/topic/Random-Forests-Algorithm)
