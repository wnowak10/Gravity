---
layout: post
title:  "How to: Use (and understand) XGBoost (and Random Forest Classifiers, more broadly)"
date:   2016-12-18 19:45:31 +0530
categories: data_science
author: "wnowak10"
multipage: true
---

If you look at [Kaggle](kaggle.com) competitions, you can quick glean that XGBoost (in Python or R) is hugely popular and successful in solving classification problems.

I am familiar with the idea of a decision tree, which is powerful in its simplicity. But the added complexity of something like XGBoost is a bit beyond me. In particular, I don't fully understand:


- [How do vanilla tree algorithms even work](#algorithms)
- Why random forests are useful and what ensembles are
- How gradient boosting applies
- What bagging is

Mysteries abound! So, here's the plan. 

1. If you need a primer on how a decision tree works, there is a lot of good extant content on this. Check out this simple [video](https://www.youtube.com/watch?v=eKD5gxPPeY0) for an explanation of the basic idea. 

2. Assuming that decision trees are understood, I am going to address the questions above, while also talking about the process through which I do so. I'll try to think through how I might go about solving the problem, and then provide research and some explanation of how the current state of the art works. 

So, let's begin... 
<br>
<br>
-----
<br>
<br>


{: id="algorithms"}
# How do vanilla tree algorithms even work?

My understanding is that vanilla decision trees work by looking at data (which can be categorical or quantitive in nature), and finding the most information-rich split (more on this later). For example, if I were trying to classify "Beverage" using "Temperature" and "Cloud cover" as classifiers (as shown in the following table)
<br>
<br>

<a>
	<img src="/images/coffeetable.png" alt="Drawing" style="width: 357px; height: 365px"/>
</a>
<br>
<br>

What would be the optimal first split for the tree?
<br>
<br>

<a>
	<img src="/images/keep-calm-and-think-about-it-17.png" alt="Drawing" style="width: 600px; height: 700px"/>
</a>
<br>
<br>

Actually, did you think about it? Hopefully.

We can see that splitting on the temp is the best call, as if temps are below 80, we can correctly bin every beverage choice. We don't even need the information provided about the cloud cover. 
<br>
<br>

{: id="table"}
<a>
	<img src="/images/classified_coffee.png" alt="Drawing" style="width: 350; height: 350"/>
</a>

<br>
<br>
This basic split shows the "tree" idea:

<br>
<br>

<a>
	<img src="/images/basic_tree.png" alt="Drawing" style="width: 550; height: 550"/>
</a>

<br>

To sum up, [Wikipedia](https://en.wikipedia.org/wiki/C4.5_algorithm) gives some semi-helpful pseudo-code that explains the decision tree process (though the gory and exciting details are absent).

1. Check for the above base cases.
2. For each attribute a, find the normalized information gain ratio from splitting on a.
3. Let a_best be the attribute with the highest normalized information gain.
4. Create a decision node that splits on a_best.
5. Recur on the sublists obtained by splitting on a_best, and add those nodes as children of node.

It still remains to be seen: how does the computer actually do this? In particular, how do they determine that attribute "a" has the highest information gain?


{% page_break %}
<br>
<br>

---
<br>
<br>

## My ideas...

Let's try to slow down a bit and be more rigorous in our discussion here. \#math! 

Assume that there are p features, and we need to determine:

- Which of the p features is best to split on? Where do we split this feature (if it is numeric or categorical with more than 2 outcomes)?

(In the example above, we had p = 2 features, and we could easily see that splitting on the temperature feature at some point between 75 and 85 was the right move.)

So we need to look at each feature and then describe, numerically, how well this split does in terms of helping us acheive our end goal of classification.

### For numeric variables:

It seems that it'd be wise to sort the data on the numeric variable. Using our example, this would be temperature. Looking at the [table](#table) above, we'd split somewhere like 55 (the median of the temperature set). If we did this, we'd find that 100% of beverages below 55 are coffee, and 60% (3/5) of beverages above 55 degrees are coffee. Ideally, in a pure split, we'd have both nodes of the split at 100% (this would mean there is no ambiguity in the classification).



## The way this actually works...



Some further references that touch on these ideas:

Thales Sehn KÃ¶rting discusses this some in this helpful [video](https://www.youtube.com/watch?v=Qdi0GBWrDO8), but I want to expound upon and clarify his ideas here. 

David MacKay's [*Infomation Theory, Inference, and Learning Algorithms*](http://www.inference.phy.cam.ac.uk/itprnn/book.pdf) gives a good explanation of information and entropy, so you can look 



<br>
<br>

---
<br>
<br>


Other useful resources:

- [XGBoost docs](http://xgboost.readthedocs.io/en/latest/model.html)
- [Vidhya](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)
- [Quora](https://www.quora.com/topic/Random-Forests-Algorithm)
