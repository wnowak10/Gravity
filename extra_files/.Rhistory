getwd()
ups <- read.table('uspsdata.txt',header = FALSE,col.names = c(1:256))
y <- read.table('uspscl.txt',header = FALSE)
train<- function(x,w,y){
j<- ncol(x)
n<- nrow(x)
theta<- array()
weight<-array()
for (i in 1:j){
newx<- sort(x[,i])
neworder<- order(x[,i])
w_cum<-array(w[neworder[1]]*y[neworder[1]])
for (k in 2:n){
w_cum[k] <- w_cum[k-1]+w[neworder[k]]*y[neworder[k]]
}
if (w_cum[1]>0){
location<- which.max(w_cum)
weight[i]<- max(w_cum)
}else{
location<- which.min(w_cum)
weight[i]<- min(w_cum)
}
theta[i]<-newx[location]
}
j<-which.max(weight)
finaltheta<-theta[j]
if (weight[j]>0){
m<-1
}else{
m<--1
}
return (list(j,finaltheta,m))
}
ups
dim(ups)
head(ups)
w=1/nrow(ups)
w
w=rep(0,nrow(ups))
w=1/nrow(ups)
w
w
w=rep(0,nrow(ups))
w
w=rep(1/nrow(ups),nrow(ups))
w
train(ups,w,y)
sort(x[,1])
x=ups
sort(x[,1])
x$X1
sort(x[,1])
order(x[,1])
neworder[1]
newx<- sort(x[,1])
neworder<- order(x[,1])
w_cum<-array(w[neworder[1]]*y[neworder[1]])
w_cum
w[neworder[1]]
w
train(ups,w,y)
source("Function for Adaboost.R")
source("Function for Adaboost.R")
# write AdaBoost algorithm
adaboost<-function(x,y,b){
alpha<-c()
allpars<-matrix(nrow = b,ncol = 3)
n<-nrow(x)
w<-c(rep((1/n),times=n))
for(i in 1:b){
pars<-train(x,w,y)
allpars[i,]<-c(pars[[1]][1],pars[[2]][1],pars[[3]][1])
label<-classify(x,pars)
wrong<-0
temp<-c()
count<-1
for(k in 1:n){
if(label[k]!=y[k]){
wrong<-wrong+w[k]
temp[count]<-k
count<-count+1
}
}
error<-wrong/sum(w)
alpha[i]<-log((1-error)/error,2)
for(m in 1:length(temp)){
k<-temp[m]
w[k]<-w[k]*(1-error)/error
}
}
list1<-list(alpha,allpars)
return(list1)
}
source("make_boost_data.R")
create_train_data(1000)
train_df=create_train_data(1000)
head(train_df)
head(train_df[,1:10])
x=train_df[,1:10]
y=train_df[,11]
adaboost(x,y,10)
b=3
allpars<-matrix(nrow = b,ncol = 3)
allpars
#boost2.R
#set wd
setwd("/Users/wnowak/wnowak10.github.io/extra_files")
set.seed(6)
# create training and test data
source('make_boost_data.R')
train_df=create_train_data(1000)
test_df=create_test_data(1000)
sum(train_df$y==1)
# train model using rpart
# single stumps. use rpart.control
library(rpart)
fit <- rpart::rpart(y ~ x1+x2+x3+x4+x5+
x6+x7+x8+x9+x10,
method="class", data=train_df,
control=rpart.control(maxdepth=1))
library(rpart.plot)
rpart.plot(fit,type=0)
# find error rates
source('predict_error.R')
predictions=predictions(fit,train_df)
train_error_rate(predictions,train_df)
test_error_rate(fit,test_df)
predictions
err=c()
alpha=c()
preds=c()
boost = function(m,train_df,test_df){
n = nrow(train_df)
#final_sum=rep(0,n)
w = rep(1/n,n) #where n is number of training obs
for(i in seq(m)){
# set training_df to take into account weights
boost_fit <-rpart(y ~ x1+x2+x3+x4+x5+
x6+x7+x8+x9+x10,
method="class", data=train_df,
weights = w,
control=rpart.control(maxdepth=1))
source('predict_error.R')
preds=c(preds,predictions(boost_fit,test_df))
predictions=predictions(boost_fit,train_df) # find predictions on train to keep working on model
# use matrix multiply to find sigma of products
err=c(err,(w%*%(predictions!=train_df$y)) / (sum(w)) )
# in line above, we are finding sigma (wi * indicator function)
# what we are doing is finding the weighted error.
# finding predictions!=train_df$y gives errors, so the inital
# round is just the raw error rate.
alpha=c(alpha,log((1-err[i])/(err[i]))) # reference first error (i=1)
# if error rate high, this approaches neg inf
# if error rate low (near 0), this approaches + inf
numeric_mismatch=as.numeric(predictions!=train_df$y)
w = w*exp(alpha[i]*numeric_mismatch) # set new weights
# if we missed prediction, w changes to w* e^alpha. if error rate
# was high, this was like e^-inf = 0...that doesnt make sense?
}
num_test_obs=dim(test_df)[1]
summ=rep(0,num_test_obs)
chunks = list()
for(i in 1:m){
chunks[[i]] = preds[(1+((i-1)*num_test_obs)) : (num_test_obs+((i-1)*num_test_obs))]
summ = summ+alpha[i]*chunks[[i]]
}
vals=ifelse(summ>0,1,-1)
return(vals)
}
sum(boost(10,train_df,train_df)==train_df$y)/nrow(train_df)
train_df=create_train_data(1000)
test_dfcreate_train_data(20000)
test_df=create_train_data(20000)
library(rpart)
fit <- rpart::rpart(y ~ x1+x2+x3+x4+x5+
x6+x7+x8+x9+x10,
method="class", data=train_df,
control=rpart.control(maxdepth=1))
library(rpart.plot)
rpart.plot(fit,type=0)
# find error rates
source('predict_error.R')
predictions=predictions(fit,train_df)
train_error_rate(predictions,train_df)
test_error_rate(fit,test_df)
predictions
err=c()
alpha=c()
preds=c()
boost = function(m,train_df,test_df){
n = nrow(train_df)
#final_sum=rep(0,n)
w = rep(1/n,n) #where n is number of training obs
for(i in seq(m)){
# set training_df to take into account weights
boost_fit <-rpart(y ~ x1+x2+x3+x4+x5+
x6+x7+x8+x9+x10,
method="class", data=train_df,
weights = w,
control=rpart.control(maxdepth=1))
source('predict_error.R')
preds=c(preds,predictions(boost_fit,test_df))
predictions=predictions(boost_fit,train_df) # find predictions on train to keep working on model
# use matrix multiply to find sigma of products
err=c(err,(w%*%(predictions!=train_df$y)) / (sum(w)) )
# in line above, we are finding sigma (wi * indicator function)
# what we are doing is finding the weighted error.
# finding predictions!=train_df$y gives errors, so the inital
# round is just the raw error rate.
alpha=c(alpha,log((1-err[i])/(err[i]))) # reference first error (i=1)
# if error rate high, this approaches neg inf
# if error rate low (near 0), this approaches + inf
numeric_mismatch=as.numeric(predictions!=train_df$y)
w = w*exp(alpha[i]*numeric_mismatch) # set new weights
# if we missed prediction, w changes to w* e^alpha. if error rate
# was high, this was like e^-inf = 0...that doesnt make sense?
}
num_test_obs=dim(test_df)[1]
summ=rep(0,num_test_obs)
chunks = list()
for(i in 1:m){
chunks[[i]] = preds[(1+((i-1)*num_test_obs)) : (num_test_obs+((i-1)*num_test_obs))]
summ = summ+alpha[i]*chunks[[i]]
}
vals=ifelse(summ>0,1,-1)
return(vals)
}
sum(boost(10,train_df,train_df)==train_df$y)/nrow(train_df)
sum(boost(10,train_df,test_df)==test_df$y)/nrow(test_df)
test_df
test_df=create_train_data(2000)
boost(10,train_df,test_df)
length(boost(10,train_df,test_df))
sum(boost(10,train_df,train_df)==train_df$y)/nrow(train_df)
test_df=create_train_data(1000)
sum(boost(10,train_df,test_df)==test_df$y)/nrow(test_df)
#set wd
setwd("/Users/wnowak/wnowak10.github.io/extra_files")
set.seed(6)
# create training and test data
source('make_boost_data.R')
train_df=create_train_data(2000)
test_df=create_test_data(10000)
sum(train_df$y==1)
# train model using rpart
# single stumps. use rpart.control
library(rpart)
fit <- rpart::rpart(y ~ x1+x2+x3+x4+x5+
x6+x7+x8+x9+x10,
method="class", data=train_df,
control=rpart.control(maxdepth=1))
library(rpart.plot)
rpart.plot(fit,type=0)
# find error rates
source('predict_error.R')
predictions=predictions(fit,train_df)
train_error_rate(predictions,train_df)
test_error_rate(fit,test_df)
predictions
err=c()
alpha=c()
#fits=c()
boost = function(m,train_df){
n = nrow(train_df)
final_sum=rep(0,n)
w = rep(1/n,n) #where n is number of training obs
for(i in seq(m)){
# set training_df to take into account weights
boost_fit <- rpart(y ~ x1+x2+x3+x4+x5+
x6+x7+x8+x9+x10,
method="class", data=train_df,
weights = w,
control=rpart.control(maxdepth=1)) # fit initial tree
#fits=c(fits,boost_fit)
source('predict_error.R')
predictions=predictions(boost_fit,train_df) # find predictions
#preds=c(preds,predictions)
# use matrix multiply to find sigma of products
err=c(err,(w%*%(predictions!=train_df$y)) / (sum(w)) )
# in line above, we are finding sigma (wi * indicator function)
# what we are doing is finding the weighted error.
# finding predictions!=train_df$y gives errors, so the inital
# round is just the raw error rate. afterwards, ....
alpha=c(alpha,log((1-err[i])/(err[i]))) # reference first error (i=1)
# if error rate high, this approaches neg inf
# if error rate low (near 0), this approaches + inf
numeric_mismatch=as.numeric(predictions!=train_df$y)
w = w*exp(alpha[i]*numeric_mismatch) # set new weights
# if we missed prediction, w changes to w* e^alpha. if error rate
# was high, this was like e^-inf = 0...that doesnt make sense?
# multiply predictions by alpha and save list of 2k #s in some list
p=predictions*alpha[i]
final_sum=final_sum+p
}
# return: sum all alpha*preds and return 1,-1 ifelse>0
final_prediction=ifelse(final_sum>0,1,-1)
return(final_prediction)
}
sum(boost(300,train_df)==train_df)/nrow(train_df) #works!!!
sum(boost(10,train_df)==train_df)/nrow(train_df) #works!!!
sum(boost(10,train_df)==test_df)/nrow(test_df) #works!!!
