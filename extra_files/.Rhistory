# make forest with various # features
# compare test accuracy of these
}
success_rates=c()
for(i in seq(10)) {
success_rates=c(success_rates,tree_prediction(10))
}
title=sprintf("Distribution of Error Rates (mean = %s)", mean(1-success_rates))
hist(1-success_rates,breaks=seq(.1,.6,.05),main=title)
abline(v=mean(1-success_rates),col="red")
abline(v=mean(1-success_rates),col="red")
# randomForest
# a function to return correlated variables
correlatedValue = function(x, r){
r2 = r**2
ve = 1-r2
SD = sqrt(ve)
e  = rnorm(length(x), mean=0, sd=SD)
y  = r*x + e
return(y)
}
# create training data
make_train_data = function( n){
#n=30 # number of variables in training set
x1 = rnorm(n) # random normal, mean 1, sd 1
x2 = correlatedValue(x=x1, r=.95) #4 other correlated features
x3  = correlatedValue(x=x1, r=.95)
x4 = correlatedValue(x=x1, r=.95)
x5 = correlatedValue(x=x1, r=.95)
# create label variable.
outcomes=c(1,0)
BER = .2 # bayes error rate of 20%
greater=which(x1>.5)
less=which(x1<.5)
y=rep(0,n)
y[greater]=sample(outcomes,length(greater),replace = T,prob = c(1-BER,BER))
y[less]=sample(outcomes,length(less),replace = T,prob = c(BER,1-BER))
# bayes error = .2 because y
# is symmetric with a bayes error of .2. if we had a<.5 and
# p(y=1|x>.5)= .5 and P(y=1|x<.5) = .7, then bayes error would be
# average of .5 and .3 = .4. ya? ya.
train_features=data.frame(x1,x2,x3,x4,x5)
train_df=data.frame(x1,x2,x3,x4,x5,y)
train_df$y=as.factor(train_df$y)
return(train_df)
}
# generate random test features
make_test_features = function(n){
#test_n=2000
test_x1 = rnorm(n)
test_x2 = correlatedValue(x=test_x1, r=.95)
test_x3  = correlatedValue(x=test_x1, r=.95)
test_x4 = correlatedValue(x=test_x1, r=.95)
test_x5 = correlatedValue(x=test_x1, r=.95)
test_features=data.frame(test_x1,test_x2,
test_x3, test_x4,
test_x5)
# rename cols so they match 'fit' model
colnames(test_features)=c('x1','x2','x3','x4','x5')
return(test_features)
}
# generate test y values, given features
make_test_label=function(features,n){
outcomes=c(1,0)
BER=.2
test_greater=which(features$x1>.5)
test_less=which(features$x1<.5)
test_y=rep(0,n)
test_y[test_greater]=sample(outcomes,length(test_greater),replace = T,prob = c(1-BER,BER))
test_y[test_less]=sample(outcomes,length(test_less),replace = T,prob = c(BER,1-BER))
return(test_y)
}
# return success classified rate for forest model
forest_prediction= function(numTrees)
{
# Classification Tree with rpart
library(randomForest)
train_df=make_train_data(30)
forestFit <- randomForest(y ~ x1 + x2 + x3 +x4+x5,
data=train_df,
ntree=numTrees,
mtry=5)#sample 5 variables
tfs=make_test_features(2000)
test_labs=make_test_label(tfs,2000)
# how does model do on test data? what is error?
p=predict(forestFit,newdata=tfs,type="response")
# codes 0s as 2s for some reason
# run this in order!
p=replace(p, p==1, 0)
p=replace(p, p==2, 1)
head(p)
head(test_labs)
return(sum(p==test_labs)/length(p)) # test success!
}
success_rates=c()
for(i in seq(100)) {
success_rates=c(success_rates,forest_prediction(5))
}
success_rates
title=sprintf("Distribution of Error Rates (mean = %s)", mean(1-success_rates))
hist(1-success_rates,breaks=seq(.1,.6,.05),
main=title,xlab="Error Rate")
abline(v=mean(1-success_rates),col="red")
correlatedValue = function(x, r){
r2 = r**2
ve = 1-r2
SD = sqrt(ve)
e  = rnorm(length(x), mean=0, sd=SD)
y  = r*x + e
return(y)
}
# above is returns correlated variable to given variable
# with level of cor given by r
# credit:
# http://stats.stackexchange.com/questions/38856/how-to-generate-correlated-random-numbers-given-means-variances-and-degree-of
# create function called predd that returns test accuracy
# arguments of function include minTreeDepth
tree_prediction= function(minTreeDepth){
#set.seed(6)
n=30 # number of variables in training set
x1 = rnorm(n) # random normal, mean 1, sd 1
x2 = correlatedValue(x=x1, r=.95) #4 other correlated features
x3  = correlatedValue(x=x1, r=.95)
x4 = correlatedValue(x=x1, r=.95)
x5 = correlatedValue(x=x1, r=.95)
# create label variable.
outcomes=c(1,0)
BER = .2 # bayes error rate of 20%
greater=which(x1>.5)
less=which(x1<.5)
y=rep(0,n)
y[greater]=sample(outcomes,length(greater),replace = T,prob = c(1-BER,BER))
y[less]=sample(outcomes,length(less),replace = T,prob = c(BER,1-BER))
# bayes error = .2 because y
# is symmetric with a bayes error of .2. if we had a<.5 and
# p(y=1|x>.5)= .5 and P(y=1|x<.5) = .7, then bayes error would be
# average of .5 and .3 = .4. ya? ya.
train_features=data.frame(x1,x2,x3,x4,x5)
train_df=data.frame(x1,x2,x3,x4,x5,y)
train_df$y=as.factor(train_df$y)
# Classification Tree with rpart
library(rpart)
library(rpart.plot)
# grow tree
fit <- rpart(y ~ x2 + x3 +x4+x5,
method="class", data=train_df
, minbucket=minTreeDepth)
# if i want to plot
#rpart.plot(fit,type=0)
#printcp(fit) # display the results
#plotcp(fit) # visualize cross-validation results
#summary(fit) # detailed summary of splits
# generate test set. ESL uses 2000 obs
test_n=2000
test_x1 = rnorm(2000)
test_x2 = correlatedValue(x=test_x1, r=.95)
test_x3  = correlatedValue(x=test_x1, r=.95)
test_x4 = correlatedValue(x=test_x1, r=.95)
test_x5 = correlatedValue(x=test_x1, r=.95)
test_features=data.frame(test_x1,test_x2,
test_x3, test_x4,
test_x5)
# rename cols so they match 'fit' model
colnames(test_features)=c('x1','x2','x3','x4','x5')
test_greater=which(test_features$x1>.5)
test_less=which(test_features$x1<.5)
test_y=rep(0,test_n)
test_y[test_greater]=sample(outcomes,length(test_greater),replace = T,prob = c(1-BER,BER))
test_y[test_less]=sample(outcomes,length(test_less),replace = T,prob = c(BER,1-BER))
# how does model do on training data? what is error?
train_p=predict(fit,newdata=train_features,type="vector")
# codes 0s as 2s for some reason
# run this in order!
train_p=replace(train_p, train_p==1, 0)
train_p=replace(train_p, train_p==2, 1)
head(train_p)
head(y)
sum(train_p==y)/length(train_p) # train success rate
# how does model do on test data? what is error?
p=predict(fit,newdata=test_features,type="vector")
# codes 0s as 2s for some reason
# run this in order!
p=replace(p, p==1, 0)
p=replace(p, p==2, 1)
head(p)
head(test_y)
return(sum(p==test_y)/length(p)) # test success!
# now construct forest annd do again
# then record # trees and test error in list to plot
# next steps
# make tree with set depth
# make forest with boot strap vs samples from data
# make forest with various # features
# compare test accuracy of these
}
success_rates=c()
for(i in seq(10)) {
success_rates=c(success_rates,tree_prediction(10))
}
title=sprintf("Distribution of Error Rates (mean = %s)", mean(1-success_rates))
hist(1-success_rates,breaks=seq(.1,.6,.05),main=title)
abline(v=mean(1-success_rates),col="red")
correlatedValue = function(x, r){
r2 = r**2
ve = 1-r2
SD = sqrt(ve)
e  = rnorm(length(x), mean=0, sd=SD)
y  = r*x + e
return(y)
}
# above is returns correlated variable to given variable
# with level of cor given by r
# credit:
# http://stats.stackexchange.com/questions/38856/how-to-generate-correlated-random-numbers-given-means-variances-and-degree-of
# create function called predd that returns test accuracy
# arguments of function include minTreeDepth
tree_prediction= function(minTreeDepth){
#set.seed(6)
n=30 # number of variables in training set
x1 = rnorm(n) # random normal, mean 1, sd 1
x2 = correlatedValue(x=x1, r=.95) #4 other correlated features
x3  = correlatedValue(x=x1, r=.95)
x4 = correlatedValue(x=x1, r=.95)
x5 = correlatedValue(x=x1, r=.95)
# create label variable.
outcomes=c(1,0)
BER = .2 # bayes error rate of 20%
greater=which(x1>.5)
less=which(x1<.5)
y=rep(0,n)
y[greater]=sample(outcomes,length(greater),replace = T,prob = c(1-BER,BER))
y[less]=sample(outcomes,length(less),replace = T,prob = c(BER,1-BER))
# bayes error = .2 because y
# is symmetric with a bayes error of .2. if we had a<.5 and
# p(y=1|x>.5)= .5 and P(y=1|x<.5) = .7, then bayes error would be
# average of .5 and .3 = .4. ya? ya.
train_features=data.frame(x1,x2,x3,x4,x5)
train_df=data.frame(x1,x2,x3,x4,x5,y)
train_df$y=as.factor(train_df$y)
# Classification Tree with rpart
library(rpart)
library(rpart.plot)
# grow tree
fit <- rpart(y ~ x1+ x2 + x3 +x4+x5,
method="class", data=train_df
, minbucket=minTreeDepth)
# if i want to plot
#rpart.plot(fit,type=0)
#printcp(fit) # display the results
#plotcp(fit) # visualize cross-validation results
#summary(fit) # detailed summary of splits
# generate test set. ESL uses 2000 obs
test_n=2000
test_x1 = rnorm(2000)
test_x2 = correlatedValue(x=test_x1, r=.95)
test_x3  = correlatedValue(x=test_x1, r=.95)
test_x4 = correlatedValue(x=test_x1, r=.95)
test_x5 = correlatedValue(x=test_x1, r=.95)
test_features=data.frame(test_x1,test_x2,
test_x3, test_x4,
test_x5)
# rename cols so they match 'fit' model
colnames(test_features)=c('x1','x2','x3','x4','x5')
test_greater=which(test_features$x1>.5)
test_less=which(test_features$x1<.5)
test_y=rep(0,test_n)
test_y[test_greater]=sample(outcomes,length(test_greater),replace = T,prob = c(1-BER,BER))
test_y[test_less]=sample(outcomes,length(test_less),replace = T,prob = c(BER,1-BER))
# how does model do on training data? what is error?
train_p=predict(fit,newdata=train_features,type="vector")
# codes 0s as 2s for some reason
# run this in order!
train_p=replace(train_p, train_p==1, 0)
train_p=replace(train_p, train_p==2, 1)
head(train_p)
head(y)
sum(train_p==y)/length(train_p) # train success rate
# how does model do on test data? what is error?
p=predict(fit,newdata=test_features,type="vector")
# codes 0s as 2s for some reason
# run this in order!
p=replace(p, p==1, 0)
p=replace(p, p==2, 1)
head(p)
head(test_y)
return(sum(p==test_y)/length(p)) # test success!
# now construct forest annd do again
# then record # trees and test error in list to plot
# next steps
# make tree with set depth
# make forest with boot strap vs samples from data
# make forest with various # features
# compare test accuracy of these
}
success_rates=c()
for(i in seq(10)) {
success_rates=c(success_rates,tree_prediction(10))
}
title=sprintf("Distribution of Error Rates (mean = %s)", mean(1-success_rates))
hist(1-success_rates,breaks=seq(.1,.6,.05),main=title)
abline(v=mean(1-success_rates),col="red")
x=rnorm(100)
y=rnorm(100)
hist(x,y)
df <- read.csv("~/Desktop/df.csv")
View(df)
hist(df$None)
ggplot(df,aes(x="None")) +
geom_histogram(data=subset(dat,Region == 'Midwest'),fill = "red", alpha = 0.2) +
ggplot(df,aes(x=None)) +
geom_histogram(data=subset(dat,Region == 'Midwest'),fill = "red", alpha = 0.2) +
geom_histogram(data=subset(dat,Region == 'Northeast'),fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(dat,Region == 'Southeast'),fill = "green", alpha = 0.2)
library(ggplot2)
ggplot(df,aes(x=None)) +
geom_histogram(data=subset(dat,Region == 'Midwest'),fill = "red", alpha = 0.2) +
geom_histogram(data=subset(dat,Region == 'Northeast'),fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(dat,Region == 'Southeast'),fill = "green", alpha = 0.2)
ggplot(df,aes(x=None)) +
geom_histogram(data=subset(df,Region == 'Midwest'),fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Southeast'),fill = "green", alpha = 0.2)
ggplot(df,aes(x=None),breaks=50) +
geom_histogram(data=subset(df,Region == 'Midwest'),fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Southeast'),fill = "green", alpha = 0.2)
ggplot(df,aes(x=None),bins=50) +
geom_histogram(data=subset(df,Region == 'Midwest'),fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Southeast'),fill = "green", alpha = 0.2)
ggplot(df,aes(x=None),bins=100) +
geom_histogram(data=subset(df,Region == 'Midwest'),fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Southeast'),fill = "green", alpha = 0.2)
ggplot(df,aes(x=None),bins=10) +
geom_histogram(data=subset(df,Region == 'Midwest'),fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Southeast'),fill = "green", alpha = 0.2)
ggplot(df,aes(x=None),binwidth=10) +
geom_histogram(data=subset(df,Region == 'Midwest'),fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Southeast'),fill = "green", alpha = 0.2)
ggplot(df,aes(x=None),binwidth=100) +
geom_histogram(data=subset(df,Region == 'Midwest'),fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Southeast'),fill = "green", alpha = 0.2)
geom_histogram(data=subset(df,Region == 'Midwest'),binwidth=10,fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Southeast'),fill = "green", alpha = 0.2)
ggplot(df,aes(x=None)) +
geom_histogram(data=subset(df,Region == 'Midwest'),binwidth=10,fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Southeast'),fill = "green", alpha = 0.2)
ggplot(df,aes(x=None)) +
geom_histogram(data=subset(df,Region == 'Midwest'),binwidth=5,fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),binwidth=5,fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Southeast'),binwidth=5,fill = "green", alpha = 0.2)
ggplot(df,aes(x=None)) +
geom_histogram(data=subset(df,Region == 'Midwest'),binwidth=1,fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),binwidth=1,fill = "blue", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Southeast'),binwidth=1,fill = "green", alpha = 0.2)
hist(df$None)
library(ggplot2)
ggplot(df,aes(x=None)) +
geom_histogram(data=subset(df,Region == 'Midwest'),binwidth=1,fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),binwidth=1,fill = "blue", alpha = 0.2)
ggplot(df,aes(x=None)) +
geom_density(data=subset(df,Region == 'Midwest'),binwidth=1,fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),binwidth=1,fill = "blue", alpha = 0.2)
ggplot(df,aes(x=None)) +
geom_density(data=subset(df,Region == 'Midwest'),fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),binwidth=1,fill = "blue", alpha = 0.2)
ggplot(df,aes(x=None)) +
geom_density(data=subset(df,Region == 'Midwest'),fill = "red", alpha = 0.2) +
geom_density(data=subset(df,Region == 'Northeast'),fill = "blue", alpha = 0.2)
ggplot(df,aes(x=None)) +
geom_density(data=subset(df,Region == 'Midwest'),binwidth=1,fill = "red", alpha = 0.2) +
geom_density(data=subset(df,Region == 'Northeast'),binwidth=1,fill = "blue", alpha = 0.2)
ggplot(df,aes(x=None)) +
geom_histogram(data=subset(df,Region == 'Midwest'),binwidth=1,fill = "red", alpha = 0.2) +
geom_histogram(data=subset(df,Region == 'Northeast'),binwidth=1,fill = "blue", alpha = 0.2)
pollution <- read.csv("~/Desktop/pollution.csv")
View(pollution)
hist(pollution$pollution)
plot(pollution$pollution,pollution$es)
#boost2.R
#set wd
setwd("/Users/wnowak/wnowak10.github.io/extra_files")
set.seed(6)
# create training and test data
source('make_boost_data.R')
train_df=create_train_data(1000)
test_df=create_test_data(1000)
sum(train_df$y==1)
# train model using rpart
# single stumps. use rpart.control
library(rpart)
fit <- rpart::rpart(y ~ x1+x2+x3+x4+x5+
x6+x7+x8+x9+x10,
method="class", data=train_df,
control=rpart.control(maxdepth=1))
library(rpart.plot)
rpart.plot(fit,type=0)
fit
446/937
#boost2.R
#set wd
setwd("/Users/wnowak/wnowak10.github.io/extra_files")
set.seed(6)
# create training and test data
source('make_boost_data.R')
train_df=create_train_data(1000)
test_df=create_test_data(1000)
sum(train_df$y==1)
# train model using rpart
# single stumps. use rpart.control
library(rpart)
fit <- rpart::rpart(y ~ x1+x2+x3+x4+x5+
x6+x7+x8+x9+x10,
method="class", data=train_df,
control=rpart.control(maxdepth=1))
library(rpart.plot)
rpart.plot(fit,type=0)
fit
# find error rates
source('predict_error.R')
predictions=predictions(fit,train_df)
train_error_rate(predictions,train_df)
test_error_rate(fit,test_df)
predictions
err=c()
alpha=c()
preds=c()
boost = function(m,train_df,test_df){
n = nrow(train_df)
#final_sum=rep(0,n)
w = rep(1/n,n) #where n is number of training obs
for(i in seq(m)){
# set training_df to take into account weights
boost_fit <-rpart(y ~ x1+x2+x3+x4+x5+
x6+x7+x8+x9+x10,
method="class", data=train_df,
weights = w,
control=rpart.control(maxdepth=1))
source('predict_error.R')
preds=c(preds,predictions(boost_fit,test_df))
predictions=predictions(boost_fit,train_df) # find predictions on train to keep working on model
# use matrix multiply to find sigma of products
err=c(err,(w%*%(predictions!=train_df$y)) / (sum(w)) )
# in line above, we are finding sigma (wi * indicator function)
# what we are doing is finding the weighted error.
# finding predictions!=train_df$y gives errors, so the inital
# round is just the raw error rate.
alpha=c(alpha,log((1-err[i])/(err[i]))) # reference first error (i=1)
# if error rate high, this approaches neg inf
# if error rate low (near 0), this approaches + inf
numeric_mismatch=as.numeric(predictions!=train_df$y)
w = w*exp(alpha[i]*numeric_mismatch) # set new weights
# if we missed prediction, w changes to w* e^alpha. if error rate
# was high, this was like e^-inf = 0...that doesnt make sense?
}
num_test_obs=dim(test_df)[1]
summ=rep(0,num_test_obs)
chunks = list()
for(i in 1:m){
chunks[[i]] = preds[(1+((i-1)*num_test_obs)) : (num_test_obs+((i-1)*num_test_obs))]
summ = summ+alpha[i]*chunks[[i]]
}
vals=ifelse(summ>0,1,-1)
return(vals)
}
sum(boost(10,train_df,train_df)==train_df$y)/nrow(train_df)
sum(boost(10,train_df,test_df)==test_df$y)/nrow(test_df)
sum(boost(20,train_df,test_df)==test_df$y)/nrow(test_df)
sum(boost(30,train_df,test_df)==test_df$y)/nrow(test_df)
sum(boost(50,train_df,test_df)==test_df$y)/nrow(test_df)
sum(boost(100,train_df,test_df)==test_df$y)/nrow(test_df)
boosting_iterations=c()
error_rates=c()
for(i in seq(20)){
boosting_iterations=c(boosting_iterations,i)
boosted_fit=boost(i,train_df,test_df)
e=1-sum(boost(i,train_df,test_df)==test_df$y)/nrow(test_df)
error_rates=c(error_rates,e)
}
plot(boosting_iterations,error_rates,col='orange')
boosting_iterations=c()
error_rates=c()
for(i in seq(50)){
boosting_iterations=c(boosting_iterations,i)
boosted_fit=boost(i,train_df,test_df)
e=1-sum(boost(i,train_df,test_df)==test_df$y)/nrow(test_df)
error_rates=c(error_rates,e)
}
plot(boosting_iterations,error_rates,col='orange')
simulation_data=data.frame(boosting_iterations,error_rates)
p <- ggplot(simulation_data, aes(boosting_iterations, error_rates))
p + geom_point()
library(ggplot2)
p <- ggplot(simulation_data, aes(boosting_iterations, error_rates))
p + geom_point()
p <- ggplot(simulation_data, aes(boosting_iterations, error_rates))
p + geom_point(col='orange')
p + geom_point(col='orange',main='ji')
p <- ggplot(simulation_data, aes(boosting_iterations, error_rates))
p + geom_point(col='orange',main='ji')
p <- ggplot(simulation_data, aes(boosting_iterations, error_rates))
p + geom_point(col='orange')+
labs(title = "New plot title")
p <- ggplot(simulation_data, aes(boosting_iterations, error_rates))
p + geom_point(col='orange')+
labs(title = "Error rate for 1 to 50 boosting iterations")
