---
layout: post
title:  "Decision tree splitting algorithms: chi-square"
date:   2016-12-23 19:45:31 +0530
categories: data_science
author: "wnowak10"
---

This is a continuation of the second part of a series where I intend to explain how decision trees work. In this, I will cover part 2 (How do vanilla tree algorithms really work?). Parts 3-5 will be in subsequent posts.

* Part 1: A beginner's take on decision trees and splitting strategies. 
* Part 2: How do vanilla tree algorithms actually work?
* Part 3: Why random forests are useful and what ensembles are
* Part 4: How gradient boosting applies
* Part 5: What bagging is

# Section 2: Categorical dependent variables

## Chi Square




<br>

<br>
<br>
<br>
<br>
<br>


- greediness?





Some further references that touch on these ideas:

Thales Sehn KÃ¶rting discusses this some in this helpful [video](https://www.youtube.com/watch?v=Qdi0GBWrDO8), but I want to expound upon and clarify his ideas here. 

David MacKay's [*Infomation Theory, Inference, and Learning Algorithms*](http://www.inference.phy.cam.ac.uk/itprnn/book.pdf) gives a good explanation of information and entropy, so you can look 



<br>
<br>

---
<br>
<br>


<a>
	<img src="/images/peewee.gif" alt="Drawing" style="width: 350; height: 350"/>
</a>

Other useful resources:

- [XGBoost docs](http://xgboost.readthedocs.io/en/latest/model.html)
- [Vidhya](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)
- [Quora](https://www.quora.com/topic/Random-Forests-Algorithm)




2016-12-18-proalgorithms.markdown